{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/conda/lib/python3.7/site-packages (0.0.25)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: textsearch in /opt/conda/lib/python3.7/site-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import contractions\n",
    "from contractions import contractions_dict\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline,DistilBertTokenizer,TFDistilBertForSequenceClassification,TFDistilBertModel, DistilBertConfig, XLNetConfig, XLNetTokenizer, TFXLNetModel, RobertaConfig, RobertaTokenizer, TFRobertaModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jigsaw_Toxic_Comment_TrainData and Jigsaw_Unintended_Bias_TrainData are loaded\n"
     ]
    }
   ],
   "source": [
    "dir = '../input/jigsaw-multilingual-toxic-comment-classification'\n",
    "\n",
    "Jigsaw_Toxic_Comment_TrainData = pd.read_csv(os.path.join(dir, 'jigsaw-toxic-comment-train.csv'))\n",
    "Jigsaw_Unintended_Bias_TrainData = pd.read_csv(os.path.join(dir, 'jigsaw-unintended-bias-train.csv'))\n",
    "\n",
    "print(f'Jigsaw_Toxic_Comment_TrainData and Jigsaw_Unintended_Bias_TrainData are loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigsaw_Toxic_Comment_TrainData has binary data for toxic variables\n",
    "# Jigsaw_Unintended_Bias_TrainData has Numeric/float data for toxic variables\n",
    "\n",
    "# Jigsaw_Toxic_Comment_TrainData\n",
    "Jigsaw_Toxic_Comment_TrainData['OtherToxic'] = Jigsaw_Toxic_Comment_TrainData[['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].apply(lambda x: np.max(x), axis=1)\n",
    "Jigsaw_Toxic_Comment_TrainData['ToxicComment'] = np.maximum(Jigsaw_Toxic_Comment_TrainData['toxic'], Jigsaw_Toxic_Comment_TrainData['OtherToxic'])\n",
    "Jigsaw_Toxic_Comment_TrainData = Jigsaw_Toxic_Comment_TrainData[['id','comment_text','ToxicComment']]\n",
    "\n",
    "# Jigsaw_Unintended_Bias_TrainData\n",
    "# Convert numeric data to binary data\n",
    "for col in ['toxic', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']:\n",
    "        Jigsaw_Unintended_Bias_TrainData[col] = Jigsaw_Unintended_Bias_TrainData[col].round()\n",
    "        \n",
    "Jigsaw_Unintended_Bias_TrainData['OtherToxic'] = Jigsaw_Unintended_Bias_TrainData[['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].apply(lambda x: np.max(x), axis=1)\n",
    "Jigsaw_Unintended_Bias_TrainData['ToxicComment'] = np.maximum(Jigsaw_Unintended_Bias_TrainData['toxic'], Jigsaw_Unintended_Bias_TrainData['OtherToxic'])\n",
    "Jigsaw_Unintended_Bias_TrainData = Jigsaw_Unintended_Bias_TrainData[['id','comment_text','ToxicComment']]\n",
    "\n",
    "# Combine Train data\n",
    "TrainData = pd.concat([Jigsaw_Toxic_Comment_TrainData, Jigsaw_Unintended_Bias_TrainData], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData = TrainData.sample(frac =.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.fillna(\"fillna\").str.lower()\n",
    "    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    # Remove Leaky features - IP addresses or user IDs\n",
    "    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    # Remove any links (whole link is replaced)\n",
    "    text = text.map(lambda x: re.sub(\"\\(http://.*?\\s\\(http://.*\\)\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\(https://.*?\\s\\(https://.*\\)\",'',str(x)))\n",
    "    # Expand contractions\n",
    "    text = text.map(lambda x: contractions.fix(x))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData[\"comment_text\"] = clean(TrainData[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer - Every transformer based model has a unique tokenization technique, unique use of special tokens. The transformer library takes care of this for us. It supports tokenization for every model which is associated with it.\n",
    "* add_special_tokens: Is used to add special character like <cls>, <sep>,<unk>, etc w.r.t Pretrained model in use. It should be always kept True\n",
    "* max_length: Max length of any sentence to tokenize, its a hyperparameter. (originally BERT has 512 max length)\n",
    "* pad_to_max_length: perform padding operation.\n",
    "    \n",
    "##### Any transformer model generally needs three input:\n",
    "* input ids: word id associated with their vocabulary\n",
    "* attention mask: Which id must be paid attention to; 1=pay attention. In simple terms, it tells the model which are original words and which are padded words or special tokens\n",
    "* token type id: It's associated with model consuming multiply sentence like Question-Answer model. It tells model about the sequence of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing input text\n",
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [],[],[]\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return [np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining DistilBERT tokonizer\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n",
    "                                                max_length=128, pad_to_max_length=True)\n",
    "\n",
    "config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212574/212574 [09:36<00:00, 368.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Returns input_ids, input_masks\n",
    "input_ids_masks = tokenize(TrainData[\"comment_text\"], tokenizer)\n",
    "labels = TrainData['ToxicComment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Train & Validation\n",
    "xtrain_id, xval_id, xtrain_mask, xval_mask, ytrain, yval = train_test_split(input_ids_masks[0], input_ids_masks[1], labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model_dense():\n",
    "#     input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "#     input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "    \n",
    "#     embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "#     # cls_token = embedding_layer[:,0,:]\n",
    "#     X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
    "#     X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "#     # X = tf.keras.layers.BatchNormalization()(cls_token)\n",
    "#     X = tf.keras.layers.Dense(50, activation='relu')(X)\n",
    "#     X = tf.keras.layers.Dropout(0.2)(X)\n",
    "#     X = tf.keras.layers.Dense(6, activation='sigmoid')(X)\n",
    "#     model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
    "\n",
    "#     for layer in model.layers[:3]:\n",
    "#       layer.trainable = False\n",
    "    \n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=1e-08, clipnorm=1.0)\n",
    "#     # loss = tf.keras.losses.binary_crossentropy(from_logits=True)\n",
    "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDisti ((None, 128, 768),)  66362880    input_token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_distil_bert_model_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 768)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            769         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 769\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model_dense():\n",
    "    input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "    input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "    \n",
    "    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "    X = tf.keras.layers.Dropout(0.2)(X)\n",
    "    X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
    "    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
    "\n",
    "    for layer in model.layers[:-1]:\n",
    "      layer.trainable = False\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=1e-08, clipnorm=1.0)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model_dense()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True)\n",
    "lr_on_pla = tf.keras.callbacks.ReduceLROnPlateau(patience=2)\n",
    "\n",
    "hist = model.fit([xtrain_id,xtrain_mask],\n",
    "                ytrain,\n",
    "                validation_data = ([xval_id, xval_mask], yval),\n",
    "                epochs=3,\n",
    "                batch_size=128,\n",
    "                callbacks=[early_stop, lr_on_pla])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
